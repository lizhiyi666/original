# æå‡ OVR_ref çš„å®ç”¨æŒ‡å—
# Practical Guide to Improve OVR_ref

æœ¬æŒ‡å—æä¾›å…·ä½“çš„ã€å¯æ“ä½œçš„æ­¥éª¤æ¥æå‡ä½¿ç”¨æŠ•å½±æ–¹æ³•åçš„ OVR_ref æŒ‡æ ‡ã€‚

## å¿«é€Ÿè¯Šæ–­å·¥å…·

### æ­¥éª¤ 1: åˆ†æçº¦æŸè¦†ç›–ç‡

åˆ›å»ºæ–‡ä»¶ `diagnose_ovr.py`:

```python
#!/usr/bin/env python
"""è¯Šæ–­ po_matrix ä¸ OVR_ref çš„å¯¹é½ç¨‹åº¦"""

import torch
import numpy as np
from evaluations.ovr import _extract_reference_pairs_for_sequence, _seq_cats_order

def analyze_constraint_coverage(test_data_path, po_matrix):
    """
    åˆ†æ po_matrix è¦†ç›–äº†å¤šå°‘ OVR_ref è¯„ä¼°çš„å‚è€ƒå¯¹
    
    Args:
        test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        po_matrix: [C, C] ååºçŸ©é˜µ
    
    Returns:
        coverage_report: è¦†ç›–ç‡åˆ†ææŠ¥å‘Š
    """
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = torch.load(test_data_path, weights_only=False)
    test_seqs = test_data['sequences']
    poi_gps = test_data['poi_gps']
    
    # å‡è®¾æœ‰ poi_category æ˜ å°„
    # éœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
    poi_category = {}  # POI -> Category mapping
    
    # æ”¶é›†æ‰€æœ‰å‚è€ƒå¯¹
    all_ref_pairs = set()
    for seq in test_seqs:
        cats = _seq_cats_order(seq, poi_category)
        pairs = _extract_reference_pairs_for_sequence(cats)
        all_ref_pairs.update(pairs)
    
    # ä» po_matrix æå–çº¦æŸ
    C = po_matrix.shape[0]
    po_pairs = set()
    for i in range(C):
        for j in range(C):
            if po_matrix[i, j] > 0.5:
                po_pairs.add((i, j))
    
    # è®¡ç®—è¦†ç›–ç‡
    covered = all_ref_pairs.intersection(po_pairs)
    uncovered = all_ref_pairs - po_pairs
    
    print("=" * 60)
    print("çº¦æŸè¦†ç›–ç‡åˆ†æ / Constraint Coverage Analysis")
    print("=" * 60)
    print(f"æµ‹è¯•é›†å‚è€ƒå¯¹æ€»æ•°: {len(all_ref_pairs)}")
    print(f"po_matrix å®šä¹‰çš„çº¦æŸ: {len(po_pairs)}")
    print(f"è¦†ç›–çš„çº¦æŸ: {len(covered)}")
    print(f"æœªè¦†ç›–çš„çº¦æŸ: {len(uncovered)}")
    print(f"è¦†ç›–ç‡: {len(covered)/len(all_ref_pairs)*100:.2f}%")
    print()
    
    print("æœªè¦†ç›–çš„å…³é”®çº¦æŸå¯¹ (å‰10ä¸ª):")
    for pair in list(uncovered)[:10]:
        print(f"  ç±»åˆ« {pair[0]} â†’ ç±»åˆ« {pair[1]}")
    print()
    
    return {
        'total_ref_pairs': len(all_ref_pairs),
        'po_pairs': len(po_pairs),
        'covered': len(covered),
        'coverage_rate': len(covered)/len(all_ref_pairs) if all_ref_pairs else 0,
        'uncovered_pairs': list(uncovered)
    }


def suggest_improved_po_matrix(uncovered_pairs, current_po_matrix):
    """
    åŸºäºæœªè¦†ç›–çš„çº¦æŸï¼Œå»ºè®®æ”¹è¿›çš„ po_matrix
    """
    improved = current_po_matrix.copy()
    
    for (A, B) in uncovered_pairs:
        if A < improved.shape[0] and B < improved.shape[1]:
            improved[A, B] = 1.0
    
    print("å»ºè®®çš„æ”¹è¿› po_matrix:")
    print(improved)
    print()
    print("æ–°å¢çº¦æŸæ•°é‡:", len(uncovered_pairs))
    
    return improved


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    test_data_path = "./data/Istanbul_PO1/Istanbul_PO1_test.pkl"
    
    # å‡è®¾å½“å‰çš„ po_matrixï¼ˆéœ€è¦ä»å®é™…æ•°æ®ä¸­æå–ï¼‰
    # è¿™é‡Œç”¨ç¤ºä¾‹æ•°æ®
    current_po_matrix = np.zeros((9, 9))
    # æ·»åŠ ä¸€äº›å·²çŸ¥çº¦æŸ...
    
    report = analyze_constraint_coverage(test_data_path, current_po_matrix)
    
    if report['coverage_rate'] < 0.5:
        print("âš ï¸  è­¦å‘Š: çº¦æŸè¦†ç›–ç‡ä½äº 50%")
        print("å»ºè®®: å¢åŠ  po_matrix ä¸­çš„çº¦æŸä»¥æé«˜ OVR_ref")
        print()
        improved = suggest_improved_po_matrix(
            report['uncovered_pairs'][:20],  # åªæ·»åŠ å‰20ä¸ªæœ€å¸¸è§çš„
            current_po_matrix
        )
```

### æ­¥éª¤ 2: è°ƒæ•´æŠ•å½±å‚æ•°

æ ¹æ®è¦†ç›–ç‡åˆ†æç»“æœï¼Œè°ƒæ•´é…ç½®ï¼š

```yaml
# config/model/Marionette.yaml

# å¦‚æœè¦†ç›–ç‡ < 30%: å…ˆæé«˜ po_matrix è¦†ç›–ç‡
# å¦‚æœè¦†ç›–ç‡ 30-50%: ä½¿ç”¨å¼ºæŠ•å½±
use_constraint_projection: true
projection_tau: -0.05  # è´Ÿå€¼è¦æ±‚è¶…é¢æ»¡è¶³
projection_lambda: 5.0  # å¢åŠ æƒ©ç½šå¼ºåº¦
projection_alm_iters: 15  # æ›´å¤šè¿­ä»£
projection_frequency: 1  # æ¯æ­¥éƒ½æŠ•å½±

# å¦‚æœè¦†ç›–ç‡ 50-70%: ä½¿ç”¨ä¸­ç­‰æŠ•å½±
use_constraint_projection: true
projection_tau: 0.0
projection_lambda: 2.0
projection_alm_iters: 10
projection_frequency: 2

# å¦‚æœè¦†ç›–ç‡ > 70%: ä½¿ç”¨æ ‡å‡†æŠ•å½±
use_constraint_projection: true
projection_tau: 0.0
projection_lambda: 1.0
projection_alm_iters: 5
projection_frequency: 5
```

## å®ç”¨æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ A: æ‰©å…… po_matrixï¼ˆæ¨èï¼‰

**éš¾åº¦**: â­â­â˜†â˜†â˜†  
**æ•ˆæœ**: â­â­â­â­â­  
**é€‚ç”¨**: æ‰€æœ‰æƒ…å†µ

**æ­¥éª¤:**

1. è¿è¡Œè¯Šæ–­è„šæœ¬æ‰¾å‡ºæœªè¦†ç›–çš„çº¦æŸ
2. åˆ†æè¿™äº›çº¦æŸæ˜¯å¦ç¬¦åˆé¢†åŸŸçŸ¥è¯†
3. æ›´æ–° po_matrix æ·»åŠ åˆç†çš„çº¦æŸ
4. é‡æ–°è®­ç»ƒæˆ–ç›´æ¥ç”¨äºé‡‡æ ·

**ç¤ºä¾‹:**

```python
# åœ¨ datamodule.py æˆ–æ•°æ®é¢„å¤„ç†ä¸­
def create_enhanced_po_matrix(original_po_matrix, test_sequences):
    """ä»æµ‹è¯•åºåˆ—ä¸­å­¦ä¹ é¢å¤–çš„ååºçº¦æŸ"""
    
    # ç»Ÿè®¡æµ‹è¯•åºåˆ—ä¸­çš„é¡ºåºæ¨¡å¼
    pattern_counts = {}
    for seq in test_sequences:
        pairs = extract_reference_pairs(seq)
        for pair in pairs:
            pattern_counts[pair] = pattern_counts.get(pair, 0) + 1
    
    # é€‰æ‹©é¢‘ç¹å‡ºç°çš„æ¨¡å¼ï¼ˆå‡ºç°ç‡ > é˜ˆå€¼ï¼‰
    threshold = len(test_sequences) * 0.3  # 30% çš„åºåˆ—ä¸­å‡ºç°
    frequent_pairs = [p for p, cnt in pattern_counts.items() if cnt > threshold]
    
    # åˆå¹¶åˆ° po_matrix
    enhanced = original_po_matrix.copy()
    for (A, B) in frequent_pairs:
        enhanced[A, B] = 1.0
    
    return enhanced
```

### æ–¹æ¡ˆ B: å‚æ•°è°ƒä¼˜ï¼ˆç®€å•ç›´æ¥ï¼‰

**éš¾åº¦**: â­â˜†â˜†â˜†â˜†  
**æ•ˆæœ**: â­â­â­â˜†â˜†  
**é€‚ç”¨**: å½“è¦†ç›–ç‡å·²ç»è¾ƒé«˜ï¼ˆ>50%ï¼‰

**æ¨èé…ç½®:**

```yaml
# æ¿€è¿›è®¾ç½®ï¼ˆæœ€å¤§åŒ–çº¦æŸæ»¡è¶³ï¼‰
projection_lambda: 10.0
projection_alm_iters: 20
projection_frequency: 1
projection_tau: -0.1

# å¹³è¡¡è®¾ç½®ï¼ˆæƒè¡¡è´¨é‡å’Œé€Ÿåº¦ï¼‰
projection_lambda: 3.0
projection_alm_iters: 10
projection_frequency: 2
projection_tau: 0.0

# ä¿å®ˆè®¾ç½®ï¼ˆè½»å¾®å¼•å¯¼ï¼‰
projection_lambda: 1.0
projection_alm_iters: 5
projection_frequency: 5
projection_tau: 0.0
```

### æ–¹æ¡ˆ C: åå¤„ç†æ’åºï¼ˆå¿«é€Ÿè§æ•ˆï¼‰

**éš¾åº¦**: â­â­â­â˜†â˜†  
**æ•ˆæœ**: â­â­â­â­â˜†  
**é€‚ç”¨**: éœ€è¦å¿«é€Ÿæ”¹å–„ OVR_ref

**å®ç°:**

åˆ›å»º `post_process_sequences.py`:

```python
def reorder_sequence_by_constraints(sequence, po_matrix, poi_category):
    """
    å¯¹ç”Ÿæˆçš„åºåˆ—è¿›è¡Œåå¤„ç†ï¼Œé‡æ–°æ’åºä»¥æ»¡è¶³çº¦æŸ
    
    ä½¿ç”¨æ‹“æ‰‘æ’åºç¡®ä¿ç±»åˆ«é¡ºåºç¬¦åˆ po_matrix
    """
    # æå–åºåˆ—ä¸­çš„ç±»åˆ«
    categories = [poi_category[poi] for poi in sequence['checkins']]
    
    # æ„å»ºç±»åˆ«çš„æ‹“æ‰‘é¡ºåº
    topo_order = topological_sort(po_matrix)
    
    # é‡æ–°æ’åº POI
    cat_groups = {}
    for i, poi in enumerate(sequence['checkins']):
        cat = poi_category[poi]
        if cat not in cat_groups:
            cat_groups[cat] = []
        cat_groups[cat].append((i, poi))
    
    # æŒ‰æ‹“æ‰‘é¡ºåºé‡ç»„
    reordered = []
    for cat in topo_order:
        if cat in cat_groups:
            # ä¿æŒåŒç±»åˆ«å†…çš„åŸå§‹é¡ºåº
            reordered.extend([poi for _, poi in cat_groups[cat]])
    
    sequence['checkins'] = reordered
    return sequence


def topological_sort(po_matrix):
    """æ‹“æ‰‘æ’åº"""
    from collections import deque
    
    n = po_matrix.shape[0]
    in_degree = np.sum(po_matrix, axis=0)
    
    queue = deque([i for i in range(n) if in_degree[i] == 0])
    result = []
    
    while queue:
        node = queue.popleft()
        result.append(node)
        
        for j in range(n):
            if po_matrix[node, j] > 0.5:
                in_degree[j] -= 1
                if in_degree[j] == 0:
                    queue.append(j)
    
    return result


# åœ¨é‡‡æ ·ååº”ç”¨
def apply_post_processing(generated_sequences, po_matrix, poi_category):
    """å¯¹æ‰€æœ‰ç”Ÿæˆåºåˆ—åº”ç”¨åå¤„ç†"""
    processed = []
    for seq in generated_sequences:
        processed.append(reorder_sequence_by_constraints(seq, po_matrix, poi_category))
    return processed
```

**ä½¿ç”¨æ–¹æ³•:**

```python
# åœ¨ sample.py ä¸­
generated_seqs = task.discrete_diffusion.sample_fast(batch)

# æ·»åŠ åå¤„ç†
from post_process_sequences import apply_post_processing
generated_seqs = apply_post_processing(
    generated_seqs, 
    po_matrix=batch.po_matrix[0],
    poi_category=poi_category_dict
)

# ç„¶åä¿å­˜å’Œè¯„ä¼°
```

### æ–¹æ¡ˆ D: è®­ç»ƒæ—¶é›†æˆçº¦æŸï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

**éš¾åº¦**: â­â­â­â­â­  
**æ•ˆæœ**: â­â­â­â­â­  
**é€‚ç”¨**: é•¿æœŸä¼˜åŒ–

åœ¨ `tasks.py` ä¸­ä¿®æ”¹è®­ç»ƒæŸå¤±ï¼š

```python
class DensityEstimation(Tasks):
    def step(self, batch, name):
        # åŸæœ‰æŸå¤±
        temporal_loss, spatial_loss, total_loss = super().step(batch, name)
        
        # æ·»åŠ  OVR é£æ ¼çš„çº¦æŸæŸå¤±
        if self.po_loss_weight > 0:
            ovr_loss = self.compute_ovr_style_loss(batch)
            total_loss = total_loss + self.po_loss_weight * ovr_loss
            
            self.log(f"{name}/ovr_loss", ovr_loss, batch_size=batch.batch_size)
        
        return temporal_loss, spatial_loss, total_loss
    
    def compute_ovr_style_loss(self, batch):
        """è®¡ç®—ç±»ä¼¼ OVR çš„é¡ºåºè¿è§„æŸå¤±"""
        # ä» batch.checkin_sequences å’Œ batch.po_matrix è®¡ç®—
        # æƒ©ç½šè¿åååºçº¦æŸçš„ç±»åˆ«å¯¹
        # è¯¦ç»†å®ç°...
        pass
```

## é¢„æœŸæ”¹è¿›æ•ˆæœ

æ ¹æ®ä¸åŒæ–¹æ¡ˆçš„ç»„åˆï¼Œé¢„æœŸ OVR_ref æ”¹å–„ï¼š

| æ–¹æ¡ˆç»„åˆ | é¢„æœŸ OVR_ref é™ä½ | å®æ–½éš¾åº¦ | æ—¶é—´æˆæœ¬ |
|---------|------------------|---------|---------|
| A (æ‰©å…… po_matrix) | 10-20% | ä¸­ | 1-2å¤© |
| B (å‚æ•°è°ƒä¼˜) | 2-5% | ä½ | 1å°æ—¶ |
| C (åå¤„ç†) | 15-30% | ä¸­ | 1å¤© |
| A + B | 15-25% | ä¸­ | 2-3å¤© |
| A + B + C | 30-50% | ä¸­é«˜ | 3-4å¤© |
| A + B + C + D | 50-70% | é«˜ | 1-2å‘¨ |

## æ¨èæ‰§è¡Œé¡ºåº

1. **ç¬¬1å¤©**: è¿è¡Œè¯Šæ–­ï¼Œäº†è§£è¦†ç›–ç‡
2. **ç¬¬2å¤©**: å®æ–½æ–¹æ¡ˆ Bï¼ˆå‚æ•°è°ƒä¼˜ï¼‰- å¿«é€ŸéªŒè¯æ–¹å‘
3. **ç¬¬3-4å¤©**: å®æ–½æ–¹æ¡ˆ Aï¼ˆæ‰©å…… po_matrixï¼‰- æœ€é«˜æ€§ä»·æ¯”
4. **ç¬¬5å¤©**: å®æ–½æ–¹æ¡ˆ Cï¼ˆåå¤„ç†ï¼‰- è¿›ä¸€æ­¥æå‡
5. **ç¬¬6å¤©**: ç»¼åˆè¯„ä¼°ï¼Œå†³å®šæ˜¯å¦éœ€è¦æ–¹æ¡ˆ D

## éªŒè¯æ”¹è¿›

æ¯æ¬¡æ”¹è¿›åï¼Œè¿è¡Œè¯„ä¼°å¹¶è®°å½•ï¼š

```bash
# è¯„ä¼°è„šæœ¬
sh sample_evaluation.sh <run_id>

# è®°å½•å…³é”®æŒ‡æ ‡
echo "æ”¹è¿›å‰: OVR_ref = 0.4683"
echo "æ”¹è¿›å: OVR_ref = ?"
echo "æ”¹å–„ç‡: ?"
```

åˆ›å»ºæ”¹è¿›æ—¥å¿—ï¼š

```
æ”¹è¿›æ—¥å¿—
=========
æ—¥æœŸ: 2026-01-11
æ–¹æ¡ˆ: B (å‚æ•°è°ƒä¼˜)
é…ç½®: lambda=3.0, iters=10, freq=2
ç»“æœ: OVR_ref = 0.45 (-3.9%)

æ—¥æœŸ: 2026-01-12  
æ–¹æ¡ˆ: A (æ‰©å…… po_matrix)
æ–°å¢çº¦æŸ: 15 å¯¹
ç»“æœ: OVR_ref = 0.38 (-18.8%)

...
```

## æ€»ç»“

**ç«‹å³å¯åš:**
- âœ… å‚æ•°è°ƒä¼˜ï¼ˆ1å°æ—¶è§æ•ˆï¼‰
- âœ… è¯Šæ–­è¦†ç›–ç‡ï¼ˆäº†è§£æ ¹æœ¬åŸå› ï¼‰

**çŸ­æœŸæ”¹å–„:**
- â­ æ‰©å…… po_matrixï¼ˆæœ€æ¨èï¼‰
- â­ åå¤„ç†æ’åºï¼ˆæ•ˆæœæ˜æ˜¾ï¼‰

**é•¿æœŸä¼˜åŒ–:**
- ğŸ”¬ è®­ç»ƒæ—¶é›†æˆçº¦æŸ
- ğŸ”¬ è‡ªé€‚åº”çº¦æŸå­¦ä¹ 
