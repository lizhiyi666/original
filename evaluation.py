#!/usr/bin/env python3
import argparse
import os
from evaluations.preprocessing import preprocessing_data
from evaluations import Get_Statistical_Metrics, run_SemLoc_task, run_EpiSim_task
# use the per-test-sequence OVR implementation
from evaluations.ovr import dataset_ovr_by_test_pairs
from evaluations.ovr import dataset_ovr_with_coverage
from evaluations.ovr import dataset_unsat_ratio_by_test_pairs
import torch
import pandas as pd
import numpy as np

dataset_file_path = os.path.dirname(os.path.abspath(__file__))


def collect_results(dataset_list, task_lists):
    def scan_result_files(file_dir):
        list_files = []
        for files in os.listdir(file_dir):
            list_files.append(files)
        return list_files

    results = []
    for dataset_name in dataset_list:
        data_frame = pd.DataFrame(columns=['method', 'mrr@5', 'mrr@10', 'ndcg@5', 'ndcg@10', 'hit@5', 'hit@10'])
        for model in task_lists:
            files = scan_result_files(f"./log/{model}")
            file = [f for f in files if dataset_name in f][0]
            with open(f'./log/{model}/{file}', 'r') as f:
                data = f.readlines()
                res_list = eval((data[-1].strip('\n').split(': ')[1])[12:-1])
                data = [item[1] for item in res_list]
                data.insert(0, model)
                data_frame.loc[len(data_frame)] = data
        results.append(data_frame)
    real_res = results[0].iloc[:, 1:]
    gene_res = results[1].iloc[:, 1:]
    absolute_differences = (gene_res - real_res).abs()
    relative_differences = absolute_differences / real_res
    MAPE = relative_differences.mean().mean()
    relative_differences_2 = np.square(relative_differences)
    MSPE = relative_differences_2.mean().mean()
    return MAPE, MSPE


def run_LocRec(task, dataset, cuda, experiment_comments, generated_only):
    print(f'preprocessing data for {task} Task')
    datasets = [dataset + '_test.pkl', f'{dataset}_generated.pkl' if experiment_comments == "" else f'{dataset}_{experiment_comments}_generated.pkl']
    datasets_prefix = [dataset + '_test_for_general', f'{dataset}_generated_for_general' if experiment_comments == "" else f'{dataset}_{experiment_comments}_generated_for_general']
    for source_file in datasets:
        preprocessing_data(f'{dataset_file_path}/data/{dataset}', source_file, task == 'NexLoc')
    LocRec_Tasks = ['DMF', 'LightGCN', 'MultiVAE', 'NeuMF', 'BPR']
    LocRec_settings = ['0', '0', '0', '0', '0']
    for index, dataset_name in enumerate(datasets_prefix):
        if generated_only and index == 0:
            continue
        for idx, model_name in enumerate(LocRec_Tasks):
            cmd = (f'python ./evaluations/run_LocRec.py --savepath {dataset_file_path}/data/{dataset} --model_name {model_name} --dataset_name {dataset_name}  --change_setting {LocRec_settings[idx]} --cuda {cuda}')
            os.system(cmd)
    LocRec_MAPE, LocRec_MSPE = collect_results(datasets_prefix, LocRec_Tasks)
    return LocRec_MAPE, LocRec_MSPE


def run_NexLoc(task, dataset, cuda, experiment_comments, generated_only):
    print(f'preprocessing data for {task} Task')
    datasets = [dataset + '_test.pkl', f'{dataset}_generated.pkl' if experiment_comments == "" else f'{dataset}_{experiment_comments}_generated.pkl']
    datasets_prefix = [dataset + '_test_for_sequential', f'{dataset}_generated_for_sequential' if experiment_comments == "" else f'{dataset}_{experiment_comments}_generated_for_sequential']
    for source_file in datasets:
        preprocessing_data(f'{dataset_file_path}/data/{dataset}', source_file, task == 'NexLoc')
    NexLoc_Tasks = ['FPMC', 'BERT4Rec', 'Caser', 'SRGNN', 'SASRec']
    NexLoc_settings = ['0', '1', '1', '1', '1']
    for index, dataset_name in enumerate(datasets_prefix):
        if generated_only and index == 0:
            continue
        for idx, model_name in enumerate(NexLoc_Tasks):
            cmd = (f'python ./evaluations/run_NexLoc.py --savepath {dataset_file_path}/data/{dataset} --model_name {model_name} --dataset_name {dataset_name}  --change_setting {NexLoc_settings[idx]} --cuda {cuda}')
            os.system(cmd)
    NexLoc_MAPE, NexLoc_MSPE = collect_results(datasets_prefix, NexLoc_Tasks)
    return NexLoc_MAPE, NexLoc_MSPE


def run_SemLoc(task, dataset, experiment_comments):
    print(f'run {task} Task')
    test_data = torch.load(f'{dataset_file_path}/data/{dataset}/{dataset}_test.pkl')
    generated_data_path = f'{dataset_file_path}/data/{dataset}/{dataset}_generated.pkl' if experiment_comments == "" else f'{dataset_file_path}/data/{dataset}/{dataset}_{experiment_comments}_generated.pkl'
    generated_data = torch.load(generated_data_path)
    test_seqs = test_data.get('sequences')
    generated_seqs = generated_data.get('sequences')
    data_all = torch.load(f'{dataset_file_path}/data/{dataset}/{dataset}_train.pkl')
    poi_label_dict = data_all['poi_category']
    SemLoc_MAPE, SemLoc_MSPE = run_SemLoc_task(test_seqs, generated_seqs, poi_label_dict)
    return SemLoc_MAPE, SemLoc_MSPE


def run_EpiSim(task, dataset, experiment_comments, init_exposed_num, exp_num, max_weeks):
    print(f'run {task} Task')
    test_data = torch.load(f'{dataset_file_path}/data/{dataset}/{dataset}_test.pkl')
    generated_data_path = f'{dataset_file_path}/data/{dataset}/{dataset}_generated.pkl' if experiment_comments == "" else f'{dataset_file_path}/data/{dataset}/{dataset}_{experiment_comments}_generated.pkl'
    generated_data = torch.load(generated_data_path)
    test_seqs = test_data.get('sequences')
    generated_seqs = generated_data.get('sequences')
    EpiSim_MAPE, EpiSim_MSPE = run_EpiSim_task(test_seqs, generated_seqs, init_exposed_num, exp_num, max_weeks)
    return EpiSim_MAPE, EpiSim_MSPE


def run_Statistical(dataset, experiment_comments):
    """
    Compute statistical metrics and the reference-based OVR (OVR_ref) for generated vs test.
    Returns:
      - JSD_Values (dict) : existing statistical metrics
      - OVR_ref (float)   : reference-based violation rate (generated vs test), may be nan if insufficient data
    """
    print(f'get Statistical Performance')
    test_data = torch.load(f'{dataset_file_path}/data/{dataset}/{dataset}_test.pkl', weights_only=False)
    generated_data_path = f'{dataset_file_path}/data/{dataset}/{dataset}_generated.pkl' if experiment_comments == "" else f'{dataset_file_path}/data/{dataset}/{dataset}_{experiment_comments}_generated.pkl'
    generated_data = torch.load(generated_data_path, weights_only=False)
    test_seqs = test_data.get('sequences')
    generated_seqs = generated_data.get('sequences')

    poi_category = test_data['poi_category']

    # adjust generated sequences marks if downstream code expects 'marks' (keep existing behavior)
    for seq in generated_seqs:
        marks_revised = []
        for i in seq.get('checkins', []):
            # robust mapping: if poi id not in poi_category, this will raise KeyError in old code; keep same mapping logic
            marks_revised.append(poi_category.get(i))
        seq['marks'] = marks_revised

    # existing statistical metrics
    JSD_Values = Get_Statistical_Metrics(test_seqs, generated_seqs)
    unsat_ratio = dataset_unsat_ratio_by_test_pairs(test_seqs, generated_seqs, poi_category, skip_nan=True)
    # compute per-test-sequence reference-based OVR: for each test sequence, extract its reference pairs
    # and compute average violation rate in the corresponding generated sequence.
    try:
        #OVR_ref = dataset_ovr_by_test_pairs(test_seqs, generated_seqs, poi_category, allow_skip=False, skip_nan=True)
        OVR_ref_skip, OVR_ref_strict, coverage = dataset_ovr_with_coverage(
        test_seqs, generated_seqs, poi_category, skip_nan=True
        )
    except Exception as e:
        # in case of errors, record nan and continue
        print(f"Warning: OVR computation failed: {e}")
        OVR_ref = float('nan')
        OVR_ref_skip = float("nan")
        OVR_ref_strict = float("nan")
        coverage = float("nan")

    return JSD_Values, OVR_ref_skip,OVR_ref_strict,coverage,unsat_ratio


def evaluation(task, dataset, cuda, results_log, experiment_comments, generated_only, init_exposed_num, exp_num, max_weeks):
    if task != "Stat":
        if task == "LocRec":
            MAPE, MSPE = run_LocRec(task, dataset, cuda, experiment_comments, generated_only)
        elif task == "NexLoc":
            MAPE, MSPE = run_NexLoc(task, dataset, cuda, experiment_comments, generated_only)
        elif task == "SemLoc":
            MAPE, MSPE = run_SemLoc(task, dataset, experiment_comments)
        elif task == "EpiSim":
            MAPE, MSPE = run_EpiSim(task, dataset, experiment_comments, init_exposed_num, exp_num, max_weeks)
        with open(results_log, "a+") as f:
            f.writelines(f"{dataset} {task} MAPE : {MAPE}, MSPE : {MSPE} \n")
    else:
        # Statistical metrics + OVR_ref
        JSD_Values, OVR_ref_skip,OVR_ref_strict,coverage,unsat_ratio = run_Statistical(dataset, experiment_comments)
        with open(results_log, "a+") as f:
            f.writelines(f"Distance: {JSD_Values['Distance']}, Radius: {JSD_Values['Radius']}, Interval: {JSD_Values['Interval']}, DailyLoc: {JSD_Values['DailyLoc']}, Category: {JSD_Values['Category']}, 'G-RANK': {JSD_Values['G-RANK']}, OVR_ref_skip: {OVR_ref_skip}, OVR_ref_strict: {OVR_ref_strict}, coverage: {coverage}, Unsat_ref: {unsat_ratio}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', default='Stat', type=str, choices=['LocRec', 'NexLoc', 'SemLoc', 'EpiSim', 'Stat'])
    parser.add_argument('--cuda', default=0, type=str)
    parser.add_argument('--datasets', default='NewYork', type=str)
    parser.add_argument('--results', default='', type=str)
    parser.add_argument("--experiment_comments", type=str, default="")
    parser.add_argument('--init_exposed_num', default=50, type=int)
    parser.add_argument('--exp_num', default=15, type=int)
    parser.add_argument('--max_weeks', default=1, type=int)
    parser.add_argument("--generated_only", default=False, action='store_true')
    args = parser.parse_args()
    if len(args.results) == 0:
        args.results = args.datasets + "_" + args.experiment_comments + "_Evaluation_results.txt" if args.experiment_comments != '' else args.datasets + "_Evaluation_results.txt"
    evaluation(args.task, args.datasets, args.cuda, args.results, args.experiment_comments, args.generated_only, args.init_exposed_num, args.exp_num, args.max_weeks)